# Лабораторные работы по предмету Фреймворки искуственного интеллекта.
Проект содержит 5 лабораторных работ по предмету Фреймворки искуственного интеллекта.
Выполнены студентом Коваленко А.И. из М8О-403Б-22

## Структура проекта

```
├── lab_1.ipynb          # KNN (K-ближайших соседей)
├── lab_2.ipynb          # Логистическая и линейная регрессия
├── lab_3.ipynb          # Решающее дерево
├── lab_4.ipynb          # Случайный лес
├── lab_5.ipynb          # Градиентный бустинг + итоговое сравнение
├── adult.csv            # Датасет для задачи классификации
├── train_data.csv       # Обучающая выборка для задачи регрессии
├── test_data.csv        # Тестовая выборка для задачи регрессии
└── README.md            # Этот файл
```

## Описание лабораторных работ

### Лабораторная работа №1: KNN (K-ближайших соседей)
- **Алгоритмы**: KNeighborsClassifier, KNeighborsRegressor
- **Задачи**: Классификация и регрессия
- **Особенности**: 
  - Создание бейзлайна и оценка качества
  - Улучшение бейзлайна (проверка гипотез, подбор гиперпараметров)
  - Имплементация собственного класса `MyKNNClassifier` и `MyKNNRegressor`

### Лабораторная работа №2: Логистическая и линейная регрессия
- **Алгоритмы**: LogisticRegression, LinearRegression
- **Задачи**: Классификация и регрессия
- **Особенности**:
  - Исследование регуляризации (L1/L2)
  - Подбор гиперпараметров через GridSearchCV
  - Имплементация собственных классов `MyLogisticRegression` и `MyLinearRegression`

### Лабораторная работа №3: Решающее дерево
- **Алгоритмы**: DecisionTreeClassifier, DecisionTreeRegressor
- **Задачи**: Классификация и регрессия
- **Особенности**:
  - Исследование критериев разбиения (gini, entropy)
  - Контроль переобучения через ограничение глубины
  - Имплементация собственных классов `MyDecisionTreeClassifier` и `MyDecisionTreeRegressor`

### Лабораторная работа №4: Случайный лес
- **Алгоритмы**: RandomForestClassifier, RandomForestRegressor
- **Задачи**: Классификация и регрессия
- **Особенности**:
  - Исследование ансамблевых методов
  - Использование решающего дерева как базового алгоритма
  - Имплементация собственных классов `MyRandomForestClassifier` и `MyRandomForestRegressor`

### Лабораторная работа №5: Градиентный бустинг
- **Алгоритмы**: GradientBoostingClassifier, GradientBoostingRegressor
- **Задачи**: Классификация и регрессия
- **Особенности**:
  - Исследование бустинга
  - Использование решающего дерева как базового алгоритма
  - Имплементация собственных классов `MyGradientBoostingClassifier` и `MyGradientBoostingRegressor`
  - **Итоговое сравнение всех алгоритмов из лабораторных работ 1-5**

## Структура каждой лабораторной работы

Каждая лабораторная работа следует единой структуре:

1. **Выбор начальных условий**
   - Выбор набора данных для классификации (adult.csv)
   - Выбор набора данных для регрессии (train_data.csv, test_data.csv)
   - Обоснование выбора метрик качества

2. **Создание бейзлайна и оценка качества**
   - Обучение моделей из sklearn
   - Оценка качества моделей по выбранным метрикам

3. **Улучшение бейзлайна**
   - Формулирование гипотез (препроцессинг, визуализация, новые признаки, гиперпараметры)
   - Проверка гипотез:
     - Обработка выбросов (IQR метод)
     - Формирование полиномиальных признаков
     - Визуализация данных (корреляционные матрицы)
     - Подбор гиперпараметров через GridSearchCV
     - Формирование новых признаков
   - Формирование улучшенного бейзлайна на основе проверенных гипотез
   - Обучение и оценка улучшенных моделей
   - Сравнение результатов с исходным бейзлайном

4. **Имплементация алгоритма машинного обучения**
   - Имплементация алгоритмов
   - Обучение имплементированных моделей
   - Оценка качества имплементированных моделей
   - Сравнение с бейзлайном
   - Добавление техник из улучшенного бейзлайна
   - Финальное сравнение результатов

## Датасеты

### adult.csv
- **Задача**: Классификация дохода (<=50K или >50K)
- **Признаки**: Демографические и профессиональные характеристики
- **Целевая переменная**: income
- **Размер**: ~32,000 записей

### train_data.csv и test_data.csv
- **Задача**: Регрессия (предсказание возраста окаменелостей)
- **Признаки**: Различные характеристики окаменелостей
- **Целевая переменная**: age
- **Разделение**: Обучающая и тестовая выборки

## Метрики качества

### Для классификации:
- **Accuracy** - доля правильно классифицированных объектов
- **F1-score** - гармоническое среднее precision и recall

### Для регрессии:
- **MSE** (Mean Squared Error) - средняя квадратичная ошибка
- **MAE** (Mean Absolute Error) - средняя абсолютная ошибка
- **R²** (коэффициент детерминации) - доля объясненной дисперсии
- **RMSE** (Root Mean Squared Error) - корень из MSE


## Результаты

| Алгоритм               | Тип задачи      | Модель                        | Accuracy | F1-score | MSE        | MAE       | R²       | RMSE      |
|------------------------|-----------------|-------------------------------|----------|----------|------------|-----------|----------|-----------|
| KNN                    | Регрессия       | sklearn                       | —        | —        | 4.13e+07   | 5019.71   | 0.8301   | 6424.74   |
| KNN                    | Регрессия       |  имплементированная           | —        | —        | 4.13e+07   | 5019.71   | 0.8301   | 6424.74   |
| KNN                    | Классификация   | sklearn                       | 0.8156   | 0.5933   | —          | —         | —        | —         |
| KNN                    | Классификация   | имплементированная            | 0.8156   | 0.5933   | —          | —         | —        | —         |
| Логистическая регрессия| Классификация   |  sklearn                      | 0.7966   | 0.3365   | —          | —         | —        | —         |
| Логистическая регрессия| Классификация   | имплементированная            | 0.6791   | 0.0719   | —          | —         | —        | —         |
| Линейная регрессия     | Регрессия       | sklearn                       | —        | —        | 7.59e+06   | 2131.31   | 0.9688   | 2754.56   |
| Линейная регрессия     | Регрессия       | имплементированная            | —        | —        | 7.59e+06   | 2131.31   | 0.9688   | 2754.56   |
| Решающее дерево        | Классификация   | sklearn                       | 0.7955   | 0.5731   | —          | —         | —        | —         |
| Решающее дерево        | Классификация   | имплементированная            | 0.7913   | 0.5698   | —          | —         | —        | —         |
| Решающее дерево        | Регрессия       | sklearn                       | —        | —        | 1.28e+07   | 2791.94   | 0.9474   | 3574.12   |
| Решающее дерево        | Регрессия       | имплементированная            | —        | —        | 1.26e+07   | 2795.33   | 0.9479   | 3556.26   |
| Случайный лес          | Классификация   | sklearn                       | 0.8529   | 0.6549   | —          | —         | —        | —         |
| Случайный лес          | Классификация   | имплементированная            | 0.8102   | 0.3739   | —          | —         | —        | —         |
| Случайный лес          | Регрессия       | sklearn                       | —        | —        | 1.29e+07   | 2566.96   | 0.9469   | 3590.63   |
| Случайный лес          | Регрессия       | имплементированная            | —        | —        | 1.42e+08   | 9462.66   | 0.4166   | 11905.36  |
| Градиентный бустинг    | Классификация   | sklearn                       | 0.8283   | 0.4824   | —          | —         | —        | —         |
| Градиентный бустинг    | Классификация   | имплементированная            | 0.7204   | 0.6021   | —          | —         | —        | —         |
| Градиентный бустинг    | Регрессия       | sklearn                       | —        | —        | 2.12e+06   | 1115.20   | 0.9913   | 1454.77   |
| Градиентный бустинг    | Регрессия       | имплементированная            | —        | —        | 2.11e+06   | 1113.22   | 0.9913   | 1453.13   |

Как видно лучшей из sklearn версий для классификации случайный лес, а для регрессии градиентный бустинг. Из имплементированных моделей лучшей как для классификация так и для регрессии оказался градиентный бустинг

